# Face Recognition & Locking with ArcFace ONNX and 5-Point Alignment

A **research-grade, CPU-first face recognition system** with **advanced face locking and behavior tracking**. Implements ArcFace embeddings with 5-point facial landmark alignment, real-time action detection, and persistent behavior logging.

Built for clarity, stability, and practical deployment on laptops without GPU acceleration.

**Based on:** *Face Recognition with ArcFace ONNX and 5-Point Alignment* by Gabriel Baziramwabo (Rwanda Coding Academy)

**Extended with:** Face Locking, Action Detection, and Behavior History Tracking

---

## ğŸ“‹ Table of Contents

1. [Features](#features)
2. [What's New: Face Locking](#whats-new-face-locking)
3. [System Requirements](#system-requirements)
4. [Installation](#installation)
5. [Project Structure](#project-structure)
6. [Quick Start](#quick-start)
7. [Detailed Usage Guide](#detailed-usage-guide)
8. [Face Locking Guide](#face-locking-guide)
9. [Pipeline Architecture](#pipeline-architecture)
10. [Troubleshooting](#troubleshooting)

---

## âœ¨ Features

### Face Recognition Core
- âœ… **CPU-Only Inference**: Runs efficiently on laptops without GPU
- âœ… **5-Point Alignment**: Haar cascade detection + MediaPipe landmark extraction
- âœ… **ArcFace ONNX Model**: 512-dimensional L2-normalized embeddings
- âœ… **Modular Design**: Each stage testable independently
- âœ… **Real-Time Recognition**: Multi-face detection with threshold-based matching
- âœ… **Threshold Evaluation**: Data-driven FAR/FRR analysis
- âœ… **Open-Set Recognition**: Rejects unknown faces automatically
- âœ… **Database Persistence**: JSON metadata + NPZ embeddings

### Face Locking (NEW)
- âœ… **Identity-Specific Tracking**: Lock onto a single enrolled identity
- âœ… **Stable Tracking**: Follow face across frames with timeout protection
- âœ… **Action Detection**: Real-time blink, movement, and expression detection
- âœ… **Behavior History**: Persistent logging of all detected actions to timestamped files
- âœ… **State Machine**: SEARCHING â†’ LOCKED â†’ LOST with automatic recovery
- âœ… **Configurable Sensitivity**: Adjust detection thresholds per use case

---

## ğŸ”’ What's New: Face Locking

Face Locking is a major extension that transforms the system from **recognition** to **tracking & behavior analysis**.

### Core Capability
When an enrolled identity appears, the system:
1. **Recognizes who they are**
2. **Locks onto their face**
3. **Tracks movements** in real-time
4. **Detects actions** (blinks, head movement, smiles)
5. **Records everything** to a timestamped history file

### Use Cases
- **Security**: Track suspects/intruders with action timeline
- **HCI Research**: Monitor user attention and engagement
- **Interview Analysis**: Record behavioral cues
- **Accessibility**: Head tracking for disabled users
- **Entertainment**: Real-time reaction detection

### Detected Actions
- **Eye Blinks**: Rapid eye closure/opening
- **Head Movement**: Left/right motion tracking
- **Smiles/Laughs**: Mouth height increase
- **Face Distance**: Moving closer/farther from camera

### Output Example
```
[00:00:05.234] BLINK       | Eye blink detected | conf=0.85 | val=0.45
[00:00:06.567] MOVE_RIGHT  | Head movement right (12.5px) | conf=0.92 | val=12.5
[00:00:07.890] SMILE       | Smile detected (ratio: 1.15) | conf=0.88 | val=1.15
```

**ğŸ‘‰ See [FACE_LOCKING_GUIDE.md](FACE_LOCKING_GUIDE.md) for detailed documentation**

---

## ğŸ–¥ï¸ System Requirements

- **Python**: 3.9+ (tested on Python 3.11)
- **OS**: macOS, Linux, or Windows
- **Webcam**: Required for camera input
- **RAM**: 2GB+ recommended
- **Storage**: ~500MB for ONNX model + dependencies

### Python Version Check

```bash
python3 --version  # Should show Python 3.9+
```

---

## ğŸ“¦ Installation

### Step 1: Clone Repository

```bash
cd /path/to/your/workspace
git clone https://github.com/ruthelvin/Face-Recog-onnx.git
cd Face-Recog-onnx
```

### Step 2: Create Virtual Environment

```bash
python3.11 -m venv .venv
```

### Step 3: Activate Virtual Environment

**macOS/Linux:**
```bash
source .venv/bin/activate
```

**Windows (PowerShell):**
```powershell
.venv\Scripts\Activate
```

### Step 4: Upgrade pip

```bash
python -m pip install --upgrade pip
```

### Step 5: Install Dependencies

Using `requirements.txt`:
```bash
pip install -r requirements.txt
```

Or manually:
```bash
pip install opencv-python numpy onnxruntime scipy tqdm mediapipe protobuf
```

### Step 6: Download ArcFace ONNX Model

```bash
# From project root directory
curl -L -o buffalo_l.zip "https://sourceforge.net/projects/insightface.mirror/files/v0.7/buffalo_l.zip/download"

unzip -o buffalo_l.zip

cp w600k_r50.onnx models/embedder_arcface.onnx

# Cleanup
rm -f buffalo_l.zip w600k_r50.onnx 1k3d68.onnx 2d106det.onnx det_10g.onnx genderage.onnx
```

âœ… **Installation complete!**

---

## ğŸ“ Project Structure

```
Face-Locking/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ camera.py                # Camera capture validation
â”‚   â”œâ”€â”€ detect.py                # Haar face detection testing
â”‚   â”œâ”€â”€ landmarks.py             # MediaPipe 5-point landmark extraction
â”‚   â”œâ”€â”€ align.py                 # Face alignment to 112Ã—112
â”‚   â”œâ”€â”€ embed.py                 # ArcFace ONNX embedding extraction
â”‚   â”œâ”€â”€ enroll.py                # Enrollment tool (build database)
â”‚   â”œâ”€â”€ evaluate.py              # Threshold evaluation (FAR/FRR analysis)
â”‚   â”œâ”€â”€ recognize.py             # Real-time recognition pipeline
â”‚   â”œâ”€â”€ haar_5pt.py              # Reusable Haar + 5pt detector class
â”‚   â”œâ”€â”€ face_lock.py             # â­ Main face locking system
â”‚   â”œâ”€â”€ action_detector.py       # â­ Action detection (blink, smile, movement)
â”‚   â””â”€â”€ face_history_logger.py   # â­ Behavior history logging
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ enroll/                  # Aligned 112Ã—112 enrollment crops (per person)
â”‚   â”‚   â”œâ”€â”€ Person1/
â”‚   â”‚   â”œâ”€â”€ Person2/
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ db/                      # Recognition database
â”‚   â”‚   â”œâ”€â”€ face_db.npz          # Embeddings (name â†’ vector)
â”‚   â”‚   â””â”€â”€ face_db.json         # Metadata
â”‚   â””â”€â”€ face_histories/          # â­ Action history logs (auto-created)
â”‚       â”œâ”€â”€ person1_history_*.txt
â”‚       â”œâ”€â”€ person2_history_*.txt
â”‚       â””â”€â”€ ...
â”œâ”€â”€ models/
â”‚   â””â”€â”€ embedder_arcface.onnx    # ArcFace model (w600k_r50)
â”œâ”€â”€ book/                        # Reference materials
â”œâ”€â”€ FACE_LOCKING_GUIDE.md        # â­ Comprehensive face locking documentation
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ README.md                    # This file
â””â”€â”€ init_project.py              # Project initialization script
```

**â­ = New files for Face Locking feature**

---

## ğŸš€ Quick Start (5 Minutes)

### Test Each Module

```bash
# 1. Verify camera access
python -m src.camera

# 2. Test face detection
python -m src.detect

# 3. Test 5-point landmarks
python -m src.landmarks

# 4. Test face alignment
python -m src.align

# 5. Test embedding extraction (requires model)
python -m src.embed
```

### Enroll & Recognize

```bash
# 6. Enroll people (run multiple times for different people)
python -m src.enroll

# 7. Evaluate threshold
python -m src.evaluate

# 8. Start live recognition
python -m src.recognize
```

### ğŸ”’ Face Locking (NEW)

```bash
# 9. Start face locking with action detection
python -m src.face_lock

# When prompted:
# 1. Select an enrolled face (e.g., "Gabi")
# 2. System searches for that face
# 3. When found â†’ lock acquired
# 4. Watch as actions are detected and logged
# 5. Press 'r' to release, 'q' to quit
# 6. History automatically saved to data/face_histories/
```

**Check History:**
```bash
ls -lh data/face_histories/
cat data/face_histories/gabi_history_*.txt
```

---

## ğŸ“– Detailed Usage Guide

### 1ï¸âƒ£ Camera Validation

**Purpose:** Verify webcam access and framerate

```bash
python -m src.camera
```

**Expected Output:**
- Live camera window opens
- Smooth motion visible
- FPS counter displayed
- Press **q** to exit

**Troubleshooting:**
- If camera doesn't open, try changing camera index in code (0â†’1 or 0â†’2)
- Check camera permissions (macOS: System Settings â†’ Privacy & Security â†’ Camera)

---

### 2ï¸âƒ£ Face Detection

**Purpose:** Validate Haar cascade face detection

```bash
python -m src.detect
```

**Expected Output:**
- Face appears in bounding box (green rectangle)
- Box follows face movement
- Press **q** to exit

---### 3ï¸âƒ£ 5-Point Landmarks

**Purpose:** Verify facial landmark extraction

```bash
python -m src.landmarks
```

**Expected Output:**
- Green bounding box around face
- 5 green points: left eye, right eye, nose, left mouth, right mouth
- Press **q** to exit

---

### 4ï¸âƒ£ Face Alignment

**Purpose:** Test 112Ã—112 alignment warping

```bash
python -m src.align
```

**Expected Output:**
- Left window: Original face with landmarks
- Right window: Aligned 112Ã—112 upright face
- Press **q** to quit
- Press **s** to save aligned image for debugging

**What's Happening:**
- Similarity transform (rotation + scale + translation) applied
- Face rotated to canonical pose
- Output always 112Ã—112 pixels

---

### 5ï¸âƒ£ Embedding Extraction

**Purpose:** Validate ArcFace ONNX model and embedding generation

```bash
python -m src.embed
```

**Expected Output:**
```
embedding dim: 512
norm(before L2): 21.85
cos(prev,this): 0.988
```

**Meaning:**
- `embedding dim: 512` â†’ ResNet-50 output size
- `norm(before L2): 21.85` â†’ Raw vector magnitude
- `cos(prev,this): 0.988` â†’ Cosine similarity between consecutive frames (should be ~0.99)

**Controls:**
- Press **q** to quit
- Press **p** to print embedding statistics

---

### 6ï¸âƒ£ Enrollment

**Purpose:** Build face recognition database

```bash
python -m src.enroll
```

**Workflow:**
1. Enter person's name (e.g., "Alice", "Bob")
2. Position face in camera
3. Capture samples using controls below
4. Press **s** to save and finalize enrollment

**Controls:**
- **SPACE** â†’ Capture one sample manually
- **a** â†’ Toggle auto-capture (every 0.25 seconds)
- **s** â†’ Save enrollment (after â‰¥15 samples recommended)
- **r** â†’ Reset new samples (keeps existing crops)
- **q** â†’ Quit without saving

**Best Practices:**
- Use consistent lighting
- Vary head angle slightly (left/right/up/down)
- Show different expressions (neutral, smile, serious)
- Capture at least 15 samples per person
- Enroll at least 10 different people for meaningful evaluation

**Output:**
- Aligned 112Ã—112 crops saved to `data/enroll/<name>/`
- Mean embedding saved to `data/db/face_db.npz`
- Metadata saved to `data/db/face_db.json`

---

### 7ï¸âƒ£ Threshold Evaluation

**Purpose:** Find optimal recognition threshold using data-driven analysis

```bash
python -m src.evaluate
```

**Expected Output:**
```
=== Distance Distributions (cosine distance = 1 - cosine similarity) ===
Genuine (same person):  n=50 mean=0.345 std=0.087 p05=0.210 p50=0.328 p95=0.502
Impostor (diff persons): n=800 mean=0.812 std=0.089 p05=0.641 p50=0.821 p95=0.951

=== Threshold Sweep ===
thr=0.10 FAR=  0.00%  FRR= 98.00%
thr=0.35 FAR=  1.25%  FRR=  8.00%
thr=0.60 FAR= 12.50%  FRR=  2.00%

Suggested threshold (target FAR 1.0%): thr=0.34 FAR=1.00% FRR=10.00%

(Equivalent cosine similarity threshold ~ 0.66, since sim = 1 - dist)
```

**Interpretation:**
- **Genuine distances** = same person pairs (should be small)
- **Impostor distances** = different person pairs (should be large)
- **FAR** (False Accept Rate) = wrongly accepting impostor as genuine
- **FRR** (False Reject Rate) = wrongly rejecting genuine face
- **Suggested threshold** = balances FAR/FRR

**Note:** Use the suggested threshold in `recognize.py` (see code: `dist_thresh=0.34`)

---

### 8ï¸âƒ£ Live Recognition

**Purpose:** Real-time face recognition with live camera input

```bash
python -m src.recognize
```

**Expected Output:**
- Live camera feed with detected faces
- Each face labeled with name or "Unknown"
- Confidence score and similarity displayed
- FPS counter in top-left

**Controls:**
- **q** â†’ Quit
- **r** â†’ Reload database from disk
- **+** â†’ Loosen threshold (more accepts, more false positives)
- **-** â†’ Tighten threshold (fewer accepts, fewer false positives)
- **d** â†’ Toggle debug overlay

**Behavior:**
- Green box + green label = recognized (accepted)
- Green box + red label = unknown (rejected)
- Confidence bar shows similarity score
- Temporal smoothing prevents flickering

---

### 9ï¸âƒ£ Face Locking & Action Detection (NEW)

**Purpose:** Lock onto an enrolled identity and track behavior in real-time

```bash
python -m src.face_lock
```

**Workflow:**
1. System shows available enrolled faces
2. You select target identity (e.g., "Gabi")
3. System enters **SEARCHING** state
4. When target appears with high confidence â†’ **LOCKED**
5. While locked, system detects and logs actions:
   - Eye blinks
   - Head movement (left/right)
   - Smiles/laughs
   - Face distance changes (closer/farther)
6. Actions logged to timestamped history file in `data/face_histories/`
7. If face disappears briefly, lock held in **LOST** state
8. Lock released if face absent too long, or press **r**

**Controls:**
- **r** â†’ Release lock manually
- **q** â†’ Quit and save history

**Example Output:**
```
Lock: LOCKED | Target: Gabi
Conf: 0.92 | Time: 15.3s
Actions: blink | move_right | smile
```

**History File Example:**
```
[00:00:05.234] BLINK       | Eye blink detected | conf=0.85 | val=0.45
[00:00:06.567] MOVE_RIGHT  | Head movement right (12.5px) | conf=0.92 | val=12.5
[00:00:07.890] SMILE       | Smile detected (ratio: 1.15) | conf=0.88 | val=1.15
```

**Check History:**
```bash
# List all history files
ls -lh data/face_histories/

# View latest session
cat data/face_histories/gabi_history_*.txt
```

**ğŸ‘‰ For detailed Face Locking documentation, see [FACE_LOCKING_GUIDE.md](FACE_LOCKING_GUIDE.md)**

---

## ğŸ—ï¸ Pipeline Architecture

### Enrollment Pipeline

```
Camera Frame
    â†“
Haar Face Detection
    â†“
MediaPipe 5-Point Landmarks
    â†“
Face Alignment (112Ã—112)
    â†“
ArcFace ONNX Embedding (512-dim)
    â†“
L2 Normalization
    â†“
Mean Pooling (multiple samples)
    â†“
Database Storage (face_db.npz)
```

### Recognition Pipeline

```
Camera Frame
    â†“
Haar Face Detection
    â†“
MediaPipe 5-Point Landmarks
    â†“
Face Alignment (112Ã—112)
    â†“
ArcFace ONNX Embedding (512-dim)
    â†“
L2 Normalization
    â†“
Cosine Distance to DB Templates
    â†“
Threshold Decision
    â†“
Accept/Reject + Display Label
```

### Face Locking Pipeline (NEW)

```
Camera Frame
    â†“
Detection & Recognition (as above)
    â†“
State Machine (SEARCHING â†’ LOCKED â†’ LOST)
    â†“
Action Detection (blink, movement, smile)
    â†“
History Logging
    â†“
Persistent File Storage
```

### Key Concepts

**5-Point Alignment:**
- Extracts: left eye, right eye, nose tip, left mouth, right mouth
- Applies similarity transform (rotation + scale + translation)
- Ensures consistent input to embedding model
- Reduces intra-class variance, improves recognition

**L2 Normalization:**
- Embedding vector divided by its L2 norm
- Results in unit vector (length = 1.0)
- Enables cosine similarity = dot product

**Cosine Distance:**
- Distance = 1 - cosine_similarity
- Range: 0 (identical) to 2 (opposite)
- Threshold ~0.34 means similarity ~0.66

---

## ğŸ› Troubleshooting

### Issue: Camera doesn't open

**Solution:**
```bash
# Try different camera index
python -m src.camera  # Try 0, 1, 2
```

**macOS Fix:**
- System Settings â†’ Privacy & Security â†’ Camera
- Add Terminal or VS Code to allowed apps
- Restart terminal

**Linux Fix:**
```bash
# Check camera permissions
ls -la /dev/video0
# Grant permissions if needed
sudo usermod -aG video $USER
```

---

### Issue: "No module named mediapipe"

**Solution:**
```bash
pip uninstall -y mediapipe
pip install mediapipe==0.10.32
```

---

### Issue: "Model not found: embedder_arcface.onnx"

**Solution:**
```bash
# Re-download and extract model
curl -L -o buffalo_l.zip "https://sourceforge.net/projects/insightface.mirror/files/v0.7/buffalo_l.zip/download"
unzip -o buffalo_l.zip
cp w600k_r50.onnx models/embedder_arcface.onnx
rm -f buffalo_l.zip w600k_r50.onnx 1k3d68.onnx 2d106det.onnx det_10g.onnx genderage.onnx
```

---

### Issue: Recognition accuracy is poor

**Solutions:**
1. Enroll more samples (target: 20-30 per person)
2. Vary lighting conditions during enrollment
3. Re-evaluate threshold with more data
4. Check alignment visually (`python -m src.align`)
5. Verify model is real ArcFace (check embedding norm values in `embed.py`)

---

### Issue: "Embedding norm is 1.0 but values seem wrong"

**Check:**
```bash
python -m src.embed
# Press 'p' to print embedding details
# Values should be between -1 and +1
# Norm should be ~1.0
```

---

## ğŸ“Š Database Format

### `face_db.npz`
- NumPy archive storing name â†’ embedding pairs
- Each embedding is 512-dimensional, L2-normalized float32
- Load with: `np.load('data/db/face_db.npz', allow_pickle=True)`

### `face_db.json`
- Metadata file with enrollment info
- Contains: timestamp, embedding dimension, enrolled names, sample counts

---

## ğŸ”§ Configuration

To adjust recognition parameters, edit `src/recognize.py`:

```python
matcher = FaceDBMatcher(
    db=db, 
    dist_thresh=0.34  # â† Change this threshold
)
```

Lower threshold = more strict (fewer false accepts, more false rejects)
Higher threshold = more lenient (more false accepts, fewer false rejects)

---

## ğŸ“ Notes

- **All embeddings are L2-normalized** (norm = 1.0)
- **Similarity matching uses cosine distance** (dot product of normalized vectors)
- **CPU-only design** ensures reproducibility and accessibility
- **Modular architecture** allows replacing any stage independently
- **No GPU required** for deployment

---

## ğŸ“š References

1. Deng et al. (2019). ArcFace: Additive Angular Margin Loss for Deep Face Recognition. CVPR 2019.
2. InsightFace Project. https://github.com/deepinsight/insightface
3. MediaPipe. https://mediapipe.dev/
4. ONNX Runtime. https://onnxruntime.ai/

---

## ğŸ“„ License

Educational use. Based on Gabriel Baziramwabo's Face Recognition course.

---

## â“ FAQ

**Q: Can I use this for production?**
A: Yes, with proper consent and legal framework. CPU performance is ~10-20 FPS.

**Q: How many faces can I enroll?**
A: Theoretically unlimited; practically tested up to 100+ identities.

**Q: Can I add GPU acceleration?**
A: Yes, change ONNX Runtime provider from `CPUExecutionProvider` to `CUDAExecutionProvider`.

**Q: What's the recognition accuracy?**
A: ~95%+ verification accuracy at 1% FAR with well-enrolled database. Depends on enrollment quality and threshold tuning.

**Q: Can I use this on mobile?**
A: ONNX Runtime supports mobile; would require porting to appropriate framework (TFLite, Core ML, etc.).

---

**Ready to build your face recognition system?** Start with Step 1 in the [Installation](#installation) section! ğŸš€
